---
title: "Decision rules for reopening from COVID-19 lockdowns"
author: "David Nguyen"
date: "`r Sys.Date()`"
output: html_document
bibliography: bib_decision_rules.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)

my_theme <- function(...) theme_minimal(base_size = 15) 
```

<!-- # Motivation -->

<!-- focus on the epidemiology and translate that into the relevant acceptance and rejection regions for re-opening. Don't worry too much about translating proposed re-opening guidelines into a hypothesis testing problem. It is enough to point out that they are trying to satisfy some objectives (NY has a very specific and concise list of objectives) -->

<!-- To safely determine when to lift COVID-19 lockdowns decision makers must know if the outbreak is sufficiently controlled and if there is adequate test and trace capacity to prevent a resurgence of cases following reopening. To support this need, criteria that should be satisfied to ensure it is safe to reopen have been proposed by policy, government, and academic entities [ex. @gottlieb_national_2020, @prevent_epidemics_adaptive_2020, @white_house_opening_2020]. A common criterion among reopening guidelines to determine if an outbreak is sufficiently controlled is a 14 day decrease in reported cases. This criterion was initially proposed in the American Enterprise Institute national reopening plan [page 3 @gottlieb_national_2020], possibly based on the reasoning that decreasing cases implies that the outbreak is controlled and the 14-day duration was chosen because most exposed individuals become infectious within 14 days [@lauer_incubation_2020]. To determine if there is adequate test and trace capacity some guidelines have proposed thresholds for the number of tests and contact tracers that are needed by population, e.g.,  a weekly average of 30 tests per 1,000 residents and 30 contact tracers per 100,000 residents ([NY Forward 2020](https://www.governor.ny.gov/sites/governor.ny.gov/files/atoms/files/NYForwardReopeningGuide.pdf)). These criteria and others are tests of the hypothesis that it unsafe (null) or safe (alternative) to reopen from lockdown. However, it is unclear what rejection region is implied by these tests, the performance of these hypothesis tests, or how they may compare to alternative tests. Consequently, it is unclear how statistically or epidemiologically justifiable these proposed reopening critera are.  -->
<!-- estimated that the 97.5 percentile of the incubation period distribution is 11.5 days (CI, 8.2 to 15.6 days)) -->

<!-- In this proposal I will address the problem of identifying when it is safe to phase out of lockdowns by developing epidemiologically justifiable hypothesis tests that can be used to set defensible criteria for reopening.  -->

<!-- 1. precisely identify the parameter region for which an epidemic is controlled enough to begin reopening -->
<!-- 2. develop a simple hypothesis test for identifying when a local epidemic is sufficiently controlled -->
<!-- 3. evaluate the performance of this hypothesis test and compare it the 14-day criteria proposed in reopening plans -->

<!-- A good decision rule should minimize the time needed to determine that the The performance of the 14-day decline in cases as a decision rule is unknown and there may exist more powerful decision rules.  -->

<!-- In this proposal I outline an approach to apply tools from sequential analysis to create statistically justified decision rules for guiding COVID-19 reopening decisions and will compare the performance of these rules with the heuristically chosen 14-day rule.  -->
<!-- For instance, what is the false detection rate of this rule? Given that an outbreak is sufficiently controlled to begin reopening, how many days will it take for -->

<!-- # Problem -->

<!-- Let $\theta \in \Theta$ be parameter that determines the evolution of an epidemic and let $\Theta_0$ be the set of parameters for which lockdown is necessary (acceptance region) and $\Theta_1$ be the set of parameters for which lockdown may be phased out (rejection region). The decision maker sequentially observes a stream of data $x_0, x_1, x_2,\ldots$ from the outbreak and must decide if the unsafe to reopen ($\theta \in \Theta_0$) or if it is safe to open ($\theta \in \Theta_1$). We seek to develop a sequential hypothesis test ($\phi(X_n)$) with a controlled type I error rate ($\alpha$)  to make this decision (i.e., $P(\text{reject } H_0 | \theta \in \Theta_0) \leq \alpha$). To solve this problem we must first define a model which describes the time-evolution of the outbreak, define the acceptance and rejection regions, construct a hypothesis tests. -->

# Model
A reasonable model for the initial phase of an outbreak (i.e. ignoring the depletion of susceptibles) is: 

\begin{align*}
Z_i & \sim NB(R(t), k)
\end{align*}

where $Z_i$ is the number of secondary cases produced by the $i$-th infected individual which follows a negative binomial distribution with mean $R(t)$ and dispersion $k$. Epidemiologically, $R(t)$ is the time-specific mean number of infections caused by an infectious individual and $k$ represents variability in infectiousness among individuals [@lloyd-smith_superspreading_2005]. Under the assumption that all $X_i$ are independent and identically distributed (iid) the total number of secondary infections is given by

\begin{align*}
X_{t+1} & \sim \Sigma^{Z_t}_{i=1}X_i \\
            & \sim NB(\Sigma^{X_t}_{i=1} R(t), \Sigma^{X_t}_{i=1} k)
\end{align*}

where the step size is chosen here to be the incubation period, 14 days. 

```{r check_dist, eval = FALSE}
# numerically test claim that the previous two distributions are the same
test <- tibble("sum(X_i)" = replicate(10000, sum(rnbinom(n = 10, mu = 2, size = 0.16))),
       "NB(r0*x_t,k*x_t)" = replicate(10000, rnbinom(n = 1, mu = 10 * 2, size = 10 * 0.16)) ) %>%
  pivot_longer(cols = 1:2,
               names_to = "distribution",
               values_to = "output")
# ggplot(test) +
#   geom_density(aes(output, fill = distribution), alpha = 0.5)

ggplot(test) +
  stat_ecdf(aes(x = output, col = distribution)) +
  labs(title = "compare distributions")
```

```{r branching_model}
episim <- function (t, i_0, r0, disp, cont, nsim) {
  
  # init list to store each simulation
  out_list <- vector("list", nsim)
  
  for (k in seq_along(out_list)) {
    # initialize data frame
    out <- tibble("time" = t
                  , "R0_true" = r0
                  , "disp_true" = disp
                  , "control" = cont
                  , "infected" = NA
                  , "simulation" = k)
    out[1, "infected"] <- i_0
    
    # branching process simulations
    for (i in 2:length(t)) {
      out[i, "infected"] <- sum(rnbinom(n = unlist(out[i-1, "infected"]), # number infected in previous time step
                                        mu = (1 - unlist(out[i-1, "control"]))  * unlist(out[i-1, "R0_true"]), # (controlled) R
                                        size = unlist(out[i-1, "disp_true"]))) # dispersion
      if(out[i, "infected"] == 0) break
    }
    # save in out_list
    out_list[[k]] <- out %>% filter(!is.na(infected))
  }
  return(bind_rows(out_list))
}

# set params and init conds
t <- 1:20
r0 <- c(rep(2, 10), rep(0.9, 10)) # lockdown starts at t = 11
disp <- 0.10 # median of posterior estimate of k for SARS-CoV-2 (Endo et al. 2020)
i_0 <- 100
cont <- 0
n <- 100

# simulate
# outbreak <- episim(t = t, i_0 = i_0, r0 = r0, disp = disp, cont = cont, nsim = n)
```

<!-- Here are some example simulations of this model. Control is implemented at the 11^th^ time step. -->

```{r plot_example_branching_proc, out.width = "50%", fig.show = 'hold', eval = FALSE}
# Plot simulations
ggplot(outbreak, aes(x = time, y = infected, group = simulation, col = as.factor(simulation) )) +
  geom_line(alpha = 0.7) +
  scale_y_log10() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Epidemic trajectories"
       , y = "log10(infected)"
       , x = "incubation periods")

outbreak %>% group_by(simulation) %>% summarize(total_infected = sum(infected)) %>% 
ggplot() +
  geom_density(aes(x = total_infected) )+
  scale_x_log10() +
  labs(title = "Outbreak size distribution" 
         ,x = "log10(total infected)")

```


<!-- ### Define acceptance and rejection region -->
<!-- *Under construction* -->
<!-- To define the regions of parameter space for which it is safe ($\Theta_0$) or unsafe ($\Theta_1$) to phase out of lockdowns we must consider the effects of control on the outbreak. The branching process model can be modified to include the effect of lockdown (population-level control) and the effect of quarantine of contacts and isolation of positive-test cases (individual-level control). -->

<!-- The effect of lockdown can be modeled with the modification $R(t) = (1-c(t) R_0$ where $c(t)$ is the proportional decrease in infectiousness caused by lockdown and $R_0$ is the expected number of secondary infections in the absence of control in a fully susceptible population. This assumes that lockdown decreases contacts, and thus infectiousness, of all individuals equally. -->

<!-- individual level control *is more complicated, need to look at the appendix of @lloyd-smith_superspreading_2005 more closely*. -->

<!-- To define acceptance and rejection regions -->

<!--   * explanation of impact of control type and magnitude on outbreak dynamics (e.g., extinction probability) -->
<!--   * identify thresholds to determine $\Theta_0$ and $\Theta_1$ -->

```{r extinction_prob, cache = TRUE} 
# per supplement of lloyd-smith et al. 2005:
# The probability that the population is extinct by the nth generation is denoted q and the pgf is denoted g(s).
# Then q = g_n(q)  as n -> inf and q is bounded [0,1]
# where g(s) = (1 + (r0/k) * (1 - s*))^(-k), for the NB model
# and g_0(s) = s, g_1(s) = g(s), ..., g_n+1(s) = g(g_n(s))

gfun <- function (s, r0, k) { # g(s; r0, k) 
  return((1 + (r0/k) * (1 - s))^(-k))
}

niteration <- 10^4
# q <- vector("numeric", length = niteration + 1)
# q[1] <- 0 # s
# 
# # set values of R to calculate extinction probability
# r0_vec <- seq(0.1, 5, by = 0.1)
# q_list <- vector("list", length = length(r0_vec))
# 
# for (i in seq_along(r0_vec)) {
#   # init vector to store values of q
#   q <- vector("numeric", length = niteration + 1)
#   q[1] <- 0 # g_0(0)
#   
#   for (j in 1:(niteration + 1)) {
#     q[j + 1] <- gfun(q[j], r0_vec[i], 0.16) # calculate g_1(0), ..., g_n(0) 
#   }
#   
#   q_list[[i]] <- tibble(n = 0:(niteration + 1),
#                         pext = q,
#                         r0 = r0_vec[i])
# }
# 
# pext_df <- bind_rows(q_list)
# 
# # verify convergence of q (all lines should asymptote)
# # ggplot(pext_df, aes(x = n, y = pext, col = r0, group = r0)) +
# #   geom_line() + viridis::scale_color_viridis()
# 
# pext_df %>%
#   group_by(r0) %>%
#   summarize(p_extinction = max(pext)) %>%
#   ggplot() +
#   geom_line(aes(x = r0, y = p_extinction), size = 1.5) +
#   geom_vline(aes(xintercept = 1), linetype = "dashed") +
#   lims(y = c(0,1)) +
#   labs(title = "Probability of stochastic extinction of outbreak",
#        x = "Basic reproductive number (R0)",
#        y = "probability of extinction (q)",
#        caption = "dispersion: k = 0.16")

# estimates of k for SARS-CoV-2 
k_vec <- c(0.54, 0.14, 6.95, # median and 90 % highest density interval from Riou and Althaus (2020) Euro Surveill
           0.1, 0.05, 0.2)   # endo et al. 2020 wellcome open research
r0_vec <- seq(0.1, 5, by = 0.1)
q_list <- vector("list", length(k_vec)*length(r0_vec))

x <- 1
for (i in seq_along(k_vec)) {
  # init vector to store values of q
  for (j in seq_along(r0_vec)) {
    q <- vector("numeric", length = niteration + 1)
    q[1] <- 0 # g_0(0)
    # calculate g_1(0), ..., g_n(0)
    for (k in 1:(niteration + 1)) {
      q[k + 1] <- gfun(q[k], r0_vec[j], k_vec[i])  
    }
  # create df and store in q_list
  q_list[[x]] <- tibble(n = 0:(niteration + 1),
                        pext = q,
                        k = k_vec[i],
                        r0 = r0_vec[j])
  x <- x + 1
  }
}

```

```{r compare_lloydsmith, eval = FALSE}
k_vec <- c(0.01, 0.1, 0.5, 1, 4, 10^6) # last one is "infinity"
r0_vec <- seq(0.1, 5, by = 0.1)
q_list <- vector("list", length(k_vec)*length(r0_vec))

x <- 1
for (i in seq_along(k_vec)) {
  # init vector to store values of q
  for (j in seq_along(r0_vec)) {
    q <- vector("numeric", length = niteration + 1)
    q[1] <- 0 # g_0(0)
    # calculate g_1(0), ..., g_n(0)
    for (k in 1:(niteration + 1)) {
      q[k + 1] <- gfun(q[k], r0_vec[j], k_vec[i])  
    }
  # create df and store in q_list
  q_list[[x]] <- tibble(n = 0:(niteration + 1),
                        pext = q,
                        k = k_vec[i],
                        r0 = r0_vec[j])
  x <- x + 1
  }
}

q_list %>% bind_rows() %>%
  group_by(k, r0) %>%
  summarize(p_extinction = max(pext)) %>%
  ggplot(aes(x = r0, y = p_extinction, group = k, col = as.factor(k) )) +
  # ggplot(aes(x = n, y = pext, col = k, group = k)) +
  geom_line(size = 2) + viridis::scale_color_viridis(discrete = T) +
  theme_minimal() +
  labs(title = "Probability of stochastic extinction of an outbreak",
       subtitle = "Successfully matches plot 2.b from Lloyd-Smith et al. 2005",
       x = "Basic reproductive number (R0)",
       y = "Probability of extinction (q)",
       col = "dispersion (k)")
  

```


```{r simulate_final_size, cache = TRUE}
# total infected after n generations
rvals <- seq(0.1, 5, by = 0.1)
finalsize_list <- vector("list", length = length(rvals))
set.seed(123)
for ( i in seq_along(rvals)) {
  finalsize_list[[i]] <- episim(t = 1:10, i_0 = 10, r0 = rvals[i], disp = disp, cont = cont, 
                                nsim = 50)
}

finalsize_df <- bind_rows(finalsize_list)

finalsize_df <-
  finalsize_df %>%
  group_by(R0_true, simulation) %>%
  summarize(total = sum(infected))
```

```{r extinction_plot, out.width = "50%", fig.show = 'hold'}
# probability of extinction
extinction_df <- 
  q_list %>% bind_rows() %>%
group_by(k, r0) %>%
summarize(p_extinction = max(pext)) %>%
mutate(estimate = case_when(k == 0.1 | k == 0.54 ~ "a.median",
                            k == 0.05 | k == 0.14 | k == 6.95 | k == 0.2 ~ "b.interval"),
       source = case_when(k == 0.05 | k == 0.1 | k == 0.2 ~ "Endo et al. (2020)",
                          k == 0.14 | k == 0.54 | k == 6.95 ~ "Riou & Althaus (2020)"))

# pal <- c("#440154FF", "#55C667FF") # set manual color palette
extinction_df %>%
  mutate(label = if_else(r0 == max(r0), paste0("k = ",k), NA_character_)) %>%
  ggplot(aes(x = r0, y = p_extinction, group = k, linetype = source)) +#, col = source, linetype = estimate)) +
  geom_line(size = 1) + 
  #viridis::scale_color_viridis(discrete = T) +
  #scale_color_manual(values = pal) +
  geom_vline(aes(xintercept = 1), linetype = "dashed", size = 1, col = "red") +
  ylim(0,1) +
  my_theme() + theme(legend.position = "none") +
  labs(title = "a.",
       x = expression(Basic ~ reproductive ~ number ~ (R[0])) ,
       y = "Probability of extinction (q)",
       col = "dispersion (k)") +
  ggrepel::geom_text_repel(aes(label = label), nudge_y = 0.05, nudge_x = 1, na.rm = TRUE)


extinction_df %>%
  #filter(source == "Endo et al. (2020)") %>%
  filter(k == 0.1) %>%
  expand_grid(z0 = c(1, 10, 50, 100)) %>%
  mutate(p_extinction = p_extinction^z0) %>%
  mutate(label = if_else(near(r0, 1.5), paste("x[0] == ~" ,z0), NA_character_)) %>%
  ggplot(aes(x = r0, y = p_extinction, group = z0)) +#, col = source, linetype = estimate)) +
  geom_line(size = 1) +
  geom_vline(aes(xintercept = 1), linetype = "dashed", size = 1, col = "red") +
  ylim(0,1) +
  my_theme() + theme(legend.position = "none") +
  labs(title = "b.",
       x = expression(Basic ~ reproductive ~ number ~ (R[0])) ,
       y = "Probability of extinction (q)",
       col = "dispersion (k)") +
  ggrepel::geom_text_repel(aes(label = label), nudge_y = 0.05, nudge_x = 1, na.rm = TRUE, parse = TRUE)


```
**Figure 1: Probability of a stochastic extinction of an outbreak for branching process model.** The probability of disease extinction depends on both on $R_0$ the average number of cases caused by single infected individual and $k$ the degree of superspreading (note that $k$ is inversely related to superspreading, such that small (large) k implies greater (lesser) contribution of superspreading as a cause of new infections). When the disease can spread in a population $(R_0 > 1)$, outbreaks that have greater degree of superspreading (smaller k) are more likely to go extinct than outbreaks with less superspreading (larger k) since most cases are caused by a highly infectious subset of individuals so the outbreak can collapse if these individuals, by chance, do not transmit much disease. When $R_0 < 1$ the probability of extinction is 1 regardless of the value of k because the number of new infections caused by existing cases is fewer than replacement by definition of $R_0$. Values of dispersion (k) used in calculations are estimates from two studies of SARS-CoV-2: solid lines are median and 95 % CrI values of posterior estimate of k from @endo_estimating_2020; dashed lines are are median and 90 % HDI values of posterior estimate of k from @riou_pattern_2020. **b.** The probability of extinction depends on the current number of infected individuals. If more individuals are currently infected, the probability of extinction is low even if $R_0$ is only slightly greater than 1. These calculations all used k = 0.1 [median posterior estimate @endo_estimating_2020].

```{r zn_plot, eval = FALSE}
# final size
finalsize_df %>%
  ggplot() +
  geom_jitter(aes(x = R0_true, y = total), alpha = 0.5) +
  scale_y_log10() +
  theme_minimal() +
  geom_vline(aes(xintercept = 1), linetype = "dashed", size = 1.5, col = "red") +
  geom_hline(aes(yintercept = 10), size = 1.5) +
  labs(title = "b. Distribution of total infections by 10th generation",
       y = "Total infections",
       x = expression(Basic ~ reproductive ~ number ~ (R[0])))

```

<!-- **Figure 2: Comparison of total cases by 10th generation.** Each point is an independent simulation. 50 simulations were run for each value of $R_0$ and k = 0.16 for all simulations. I don't think this figure is necessary. -->

<!-- should use the two median estimates of k for the size at n-th generation plot -->

# Sequential Testing

### Defining a sequential test statistic
Making a slight modification to @phatarfod_sequential_1965 to allow for composite hypothesis testing of dependent observations let 

\begin{align*}
S_0 &= \log \left( \frac{p(x_0 | \theta \leq \theta_0 )}{p(x_0 | \theta > \theta_0)} \right) \\
S_r &= \log \left( \frac{p(x_r | x_{r-1}, \theta \leq \theta_0 )}{p(x_r | x_{r-1}, \theta > \theta_0)}\right), r \geq 1 \\
\end{align*}

then the sequential probability ratio test (SPRT) for the composite hypothesis test $H_0: \theta > \theta_0 \text{ against } H_1: \theta \leq \theta_0$ is

\begin{align*}
\phi(x_n) = 
    \begin{cases}
      \text{reject } H_0  & \Sigma^{n}_{r = 0} S_r > \log(A)\\
      \text{accept } H_0 & \Sigma^{n}_{r = 0} S_r \leq \log(B)
    \end{cases}
\end{align*}

The thresholds are set following Wald's method such that $A \sim \frac{1-\beta}{\alpha}$ and $B \sim \frac{\beta}{1-\alpha}$ where $\alpha$ and $\beta$ is the probabilities of type I and II error, respectively.

In our case, the parameter ($\theta$) of interest is $R(t)$. When $S_r > \log(A)$ the decision maker concludes that it is unsafe to reopen and when $S_r \leq \log (B)$. This should have an advantage over the criteria that have been proposed on purely heuristic grounds since the decision maker is able to set choose the type I and II error rates whereas the error rates of the proposed criteria are uncontrolled and unknown. Another likely justification is that the SPRT may be the optimal test for this problem class (it is for the case where observations are iid, not sure about the markov dependence case).


```{r likelihood, results = 'hide'}
# if it is true that for X_i ~ NB(mu, k) iid that sum(X_i) ~ NB(sum(mu), sum(k)) then the P( x_{t+1} | x_t, theta ) is 
# dnbinom(x = x1, mu = x0 * r0, size = x0 * disp) because the X_i are iid

objfn <- function(r, x1, x0, dispersion) {
  # likelihood( x1 | x0, r, dispersion )
  dnbinom(x = x1, mu = x0*r, size = x0*dispersion)
}


#  find restricted MLE under h_0 or h_1
likelihood <- function (x_prev, x_now, disp, interval = c(), guess = 1.5) {
  opt <- optim(par = guess, fn = objfn, control = list(fnscale = -1), 
      method = "Brent", lower = interval[1], upper = interval[2],
      x0 = x_prev, x1 = x_now, dispersion = disp)
  return(opt$value)
}

#likelihood(x_prev = 10, x_now = 20, disp = 0.16, interval = c(1, 2.5))

```

```{r calculate_sprt, results = 'hide', cache = TRUE}
set.seed(123)
df_break_high <- episim(t = t, i_0 = 100, r0 = r0, disp = disp, cont = cont, nsim = 100)
df_break_low <- episim(t = t, i_0 = 10, r0 = r0, disp = disp, cont = cont, nsim = 100)

calc_sr <- function(data, accept_reg = c(1 + 10^-6, 2.5), reject_reg = c(0, 1)) {
  
  # initialize empty columns for likelihood calculations
  data$lik0 <- NA
  data$lik1 <- NA
  
  # parameter space
  theta0 <- accept_reg
  theta1 <- reject_reg
  
  # use midpoint of interval as init guess for optim (Brent's method)
  guess0 <- accept_reg[1] + (accept_reg[2] - accept_reg[1]) / 2 
  guess1 <- reject_reg[1] + (reject_reg[2] - reject_reg[1]) / 2 
  
  sim_number <- 1
  for (i in 2:nrow(data)) {
    
    if (data[i, "simulation"] == sim_number + 1) {
      # skip calculation of likelihood for the first observation in each simluation
      sim_number <- sim_number + 1
      next
    }
    
    # observed states and known pars
    x0 <- unlist(data[i-1, "infected"])
    x1 <- unlist(data[i, "infected"])
    disp <- unlist(data[i-1, "disp_true"])
    
    # likelihood calculation
    data[i,"lik0"] <- likelihood(x_prev = x0, x_now = x1, disp = disp,  # p(x_now | x_prev, disp, theta \in Theta_0)
                                   interval = theta0, guess = guess0)
    data[i,"lik1"] <- likelihood(x_prev = x0, x_now = x1, disp = disp,  # p(x_now | x_prev, disp, theta \in Theta_1)
                                   interval = theta1, guess = guess1)
  }
  
  return(data)
}

df_break_high <- calc_sr(df_break_high)
df_break_low <- calc_sr(df_break_low) 

break_high_sprt <- df_break_high %>%
  group_by(simulation) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.05)/ 0.05),
         b = log(0.05/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))

break_low_sprt <- df_break_low %>%
  group_by(simulation) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.05)/ 0.05),
         b = log(0.05/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))


calc_reset_sr <- function(data) {
  
  # init vectors to hold reset sprt (cumulative sr) and decision
  sprt_reset <- vector("numeric", length = nrow(data))
  decision_reset <- vector("character", length = nrow(data))
  
  # init data
  sr <- data$sr
  a <- unique(data$a)
  b <- unique(data$b)
  
  # both models are equally likely for first observation
  sprt_reset[1] <- 0
  decision_reset[1] <- "?"
  
  sim_number <- 1
  for (i in 2:nrow(data)) {
    
    if (data[i, "simulation"] == sim_number + 1) {
      # skip calculation of resetting sprt for the first observation in each simluation
      sim_number <- sim_number + 1
      sprt_reset[i] <- 0
      decision_reset[i] <- "?"
      next
    }
    
    # if no decision has been made, update SPRT
    # otherwise, reset the SPRT
    ifelse(decision_reset[i] == "?",
           sprt_reset[i] <- sprt_reset[i-1] + sr[i],
           sprt_reset[i] <- sr[i])
    
    # if SPRT has crossed bounds, make decision
    # otherwise continue sampling
    ifelse(sprt_reset[i] < b
           ,{decision_reset[i] <- "H0"}#; sprt_reset[i+1] <- sr[i+1]; next}
           ,ifelse(sprt_reset[i] > a
                   ,{decision_reset[i] <- "H1"}#; sprt_reset[i+1] <- sr[i+1]; next}
                   ,{decision_reset[i] <- "?"})
    )
  }
  
  # attach reset sprt calculations and decisions to df
  data$sprt_reset <- sprt_reset
  data$decision_reset <- decision_reset
  
  return(data)
}

break_high_sprt <- calc_reset_sr(break_high_sprt)
break_low_sprt <- calc_reset_sr(break_low_sprt)
# test_sprt
```

```{r plot_sprt, out.width = "50%", fig.show = 'hold'}
break_low_sprt %>%
  filter(simulation %in% 1:10) %>%
  ggplot() +
  geom_line(aes(x = time, y = sprt, group = simulation)) +
  geom_point(aes(x = time, y = sprt, group = simulation, col = decision), size = 2) +
  geom_hline(aes(yintercept = a), linetype = "dashed") +
  geom_hline(aes(yintercept = b), linetype = "dashed") +
  labs(title = "SPRT without resetting",
       # caption = "Previous confirmation of H0 overwhelms new information that is consistent with H1",
       y = expression(Test ~ statistic ~ (S[t])),
       x = "Generation number (t)") +
  my_theme()

break_low_sprt %>%
  filter(simulation %in% 1:10) %>%
  ggplot() +
  geom_line(aes(x = time, y = sprt_reset, group = simulation)) +
  geom_point(aes(x = time, y = sprt_reset, group = simulation, col = decision_reset), size = 2) +
  geom_hline(aes(yintercept = a), linetype = "dashed") +
  geom_hline(aes(yintercept = b), linetype = "dashed") +
  labs(title = "SPRT with resetting",       
       y = expression(Test ~ statistic ~ (S[t])),
       x = "Generation number (t)") +
  my_theme()
```
**Figure 2: Sequentially testing ** 
$\boldsymbol{H_0: R_t > 1}$ 
**versus** 
$\boldsymbol{H_1: R_t \leq 1}$**.**
a. $H_0$ is rapidly accepted by the SPRT; however, it is slow to register a change in $R_t$ at $t = 11$ and falsely accepts $H_0$ when $H_1: R_t < 1$ is true because of the previous evidence that accumulated against $H-1$. b. When the SPRT is restarted after every decision is made about $H_0$ versus $H_1$ the test is able to quickely detect changes. However, it is unclear what repeatedly restarting tests does to the type I and II error rates ($\alpha, \beta$) set by the decision maker.

# Sequential testing case studies
The general format of these case studies is to investigate the performance of the reset SPRT for detecting changes in $R_t$ for the negative binomial model. In all cases I study how the RSPRT performs when $R_t$ starts out greater than $1$ and then drops below $1$. For each scenario I investigated the effect of the initial size of the infected population. 

<!-- Generally, I find that small starting populations result in worse performance, presumably because the signals generated by shifts in $R_t$ are obscured by demographic stochasticity which is more influential for small populations. -->

### Case 1: Breakpoint drop in $R_t$

Here, I assume that mean cases generated by an individual follows

\begin{align*}
R_t = 
    \begin{cases}
      2  & \text{when} & t = 1, ..., 10\\
      0.9 & \text{when} & t = 11, ..., 20
    \end{cases}
\end{align*}

and test the hypothesis $H_0: R_t > 1 \text{ against } H_1: R_t \leq 1$. This means that $H_0$ is true when $t = 1, ..., 10$ and $H_1$ is true when $t = 11, ... ,20$.

```{r decision_lineplot, warning = FALSE, out.width = "50%", fig.show = 'hold'}
break_low_sprt %>%
  ggplot() +
  geom_line(aes(x = time, y = infected, group = simulation)) +
  geom_point(aes(x = time, y = infected, group = simulation, col = decision_reset)) +
  geom_vline(aes(xintercept = 11), linetype = "dashed", size = 1.5) +
  scale_y_log10(breaks = c(1, 10, 100, 1000, 10000)) +
  labs(title = "a",#title = "Outbreak sizes and decisions",
       y = expression(New~infections~(x[t])),
       x = "Outbreak generation (t)",
       col = "Decision") + 
  my_theme()

break_high_sprt %>%
  ggplot() +
  geom_line(aes(x = time, y = infected, group = simulation)) +
  geom_point(aes(x = time, y = infected, group = simulation, col = decision_reset)) +
  geom_vline(aes(xintercept = 11), linetype = "dashed", size = 1.5) +
  scale_y_log10(breaks = c(1, 10, 100, 1000, 10000)) +
  labs(title = "b",#title = "Outbreak sizes and decisions",
       y = expression(New~infections~(x[t])),
       x = "Outbreak generation (t)",
       col = "Decision") + 
  my_theme()
  
# threshold <- 5000
# break_low_sprt %>%
#   group_by(simulation) %>%
#   filter(max(infected) >= threshold) %>%
#   ggplot() +
#   geom_line(aes(x = time, y = infected, group = simulation)) +
#   geom_point(aes(x = time, y = infected, group = simulation, col = decision_reset)) +
#   scale_y_log10(breaks = c(1, 10, 100, 1000, 10000)) +
#   labs(title = "Outbreak sizes and decisions",
#        x = "outbreak generation (t)",
#        col = "decision") + 
#   my_theme()


```

```{r decision_barplot, out.width = "50%", fig.show = 'hold'}
break_low_sprt %>%
  group_by(time) %>%
  count(decision_reset) %>%
  complete(nesting(decision_reset = c("?", "H0", "H1")), fill = list(n = 0)) %>%
  ggplot() +
  geom_bar(aes(x = time, y = n, fill = decision_reset), stat = "identity", position = "fill") +
  geom_vline(aes(xintercept = 11), linetype = "dashed", size = 1.5) +
  labs(title = "c",#title = "Conclusions of of reset SPRT over time",
       #subtitle = "H0 is true from t = 1:10; H1 is true from t =11:20",
       y = "Proportion of simulations",
       x = "Outbreak generation (t)",
       fill = "Decision") +
  my_theme()

break_high_sprt %>%
  group_by(time) %>%
  count(decision_reset) %>%
  complete(nesting(decision_reset = c("?", "H0", "H1")), fill = list(n = 0)) %>%
  ggplot() +
  geom_bar(aes(x = time, y = n, fill = decision_reset), stat = "identity", position = "fill") +
  geom_vline(aes(xintercept = 11), linetype = "dashed", size = 1.5) +
  labs(title = "d",#title = "Conclusions of of reset SPRT over time",
       #subtitle = "H0 is true from t = 1:10; H1 is true from t =11:20",
       y = "Proportion of simulations",
       x = "Outbreak generation (t)",
       fill = "Decision") +
  my_theme()

# break_low_sprt %>%
#   group_by(simulation) %>%
#   mutate(bigenough = ifelse(max(infected) >= threshold , TRUE, FALSE)) %>%
#   ungroup() %>%
#   filter(bigenough == TRUE) %>% #View()
#   group_by(time, bigenough) %>%
#   count(decision_reset) %>% 
#   complete(nesting(decision_reset = c("?", "H0", "H1")), fill = list(n = 0)) %>% #View()
#   ggplot() +
#   geom_bar(aes(x = time, y = n, fill = decision_reset), stat = "identity", position = "fill") +
#   labs(title = "Conclusions of of reset SPRT over time",
#        subtitle = "H0 is true from t = 1:10; H1 is true from t =11:20",
#        caption = "Subset of simulations where max(Xi >= 5000)",
#        y = "proportion of simulations",
#        x = "outbreak generation (t)",
#        fill = "decision") +
#   my_theme()

```
**Figure 3.**

### Case 2: Linearly decreasing $R_t$

How do the sequential tests perform when $R_t$ decreases linearly $R_t = 2.1 - 0.1t$ where $t = 1, ... , 20$. 

We test the hypothesis that $H_0: R_t > 1-c \text{ against } H_1: R_t \leq 1-c$. That is, when is transmission low enough that repoening will not result in $R > 1$. In this case $c = 0.3$ so the threshold is $0.7$. This means that $H_1$ is true when $t = 13, ... , 20$.

```{r sim_linear_rt, cache = TRUE}
r_linear <- seq(2, 0.1, by = -0.1)
thresh_R <- 0.7
rejection <- c(0, thresh_R)
acceptance <- c(thresh_R + 10^-6, 2.5)

set.seed(123)
df_lin_high <- episim(t = t, i_0 = 10^2, r0 = r_linear, disp = disp, cont = cont, nsim = 100)
df_lin_low <- episim(t = t, i_0 = 10^1, r0 = r_linear, disp = disp, cont = cont, nsim = 100)
```

```{r calc_sprt_lineardecrease}
lin_high_sprt <- calc_sr(df_lin_high, accept_reg = acceptance, reject_reg = rejection)
lin_low_sprt <- calc_sr(df_lin_low, accept_reg = acceptance, reject_reg = rejection)

lin_high_sprt <- lin_high_sprt %>%
  group_by(simulation) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.05)/ 0.05),
         b = log(0.05/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))

lin_low_sprt <- lin_low_sprt %>%
  group_by(simulation) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.05)/ 0.05),
         b = log(0.05/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))

lin_high_sprt <- calc_reset_sr(lin_high_sprt)
lin_low_sprt <- calc_reset_sr(lin_low_sprt)


```

```{r plot_lin_rt, warning = FALSE, out.width = "50%", fig.show = 'hold'}
lin_low_sprt %>%
  ggplot() +
  geom_line(aes(x = time, y = infected, group = simulation)) +
  geom_point(aes(x = time, y = infected, group = simulation, col = decision_reset)) +
  geom_vline(aes(xintercept = which(r_linear == 0.7)), linetype = "dashed", size = 1.5) +
  geom_vline(aes(xintercept = which(r_linear == 1)), size = 1.5) +
  scale_y_log10() +
  viridis::scale_color_viridis(discrete = TRUE) +
  labs(title = "a",#title = "Outbreak sizes and decisions",
       y = expression(New~infections~(x[t])),
       x = "Outbreak generation (t)",
       col = "Decision") + 
  my_theme()

lin_high_sprt %>%
  ggplot() +
  geom_line(aes(x = time, y = infected, group = simulation)) +
  geom_vline(aes(xintercept = which(r_linear == 0.7)), linetype = "dashed", size = 1.5) +
  geom_vline(aes(xintercept = which(r_linear == 1)), size = 1.5) +
  scale_y_log10() +
  labs(title = "b",#title = "Outbreak sizes and decisions",
       y = expression(New~infections~(x[t])),
       x = "Outbreak generation (t)",
       col = "Decision") + 
  my_theme()
```

```{r plot_performance_lin_rt, out.width='50%', fig.show = 'hold'}
lin_low_sprt %>%
  group_by(time) %>%
  count(decision_reset) %>%
  complete(nesting(decision_reset = c("?", "H0", "H1")), fill = list(n = 0)) %>%
  ggplot() +
  geom_bar(aes(x = time, y = n, fill = decision_reset), stat = "identity", position = "fill", width = 1) +
  geom_vline(aes(xintercept = which(r_linear == 0.7)), linetype = "dashed", size = 1.5) +
  geom_vline(aes(xintercept = which(r_linear == 1)), size = 1.5) +
  viridis::scale_fill_viridis(discrete = TRUE, alpha = 0.7) +
  labs(title = "c",#title = "Conclusions of of reset SPRT over time",
       #subtitle = "H0 is true from t = 1:10; H1 is true from t =11:20",
       y = "Proportion of simulations",
       x = "Outbreak generation (t)",
       fill = "Decision") +
  my_theme()

lin_high_sprt %>%
  group_by(time) %>%
  count(decision_reset) %>%
  complete(nesting(decision_reset = c("?", "H0", "H1")), fill = list(n = 0)) %>%
  ggplot() +
  geom_bar(aes(x = time, y = n, fill = decision_reset), stat = "identity", position = "fill", width = 1) +
  geom_vline(aes(xintercept = which(r_linear == 0.7)), linetype = "dashed", size = 1.5) +
  geom_vline(aes(xintercept = which(r_linear == 1)), size = 1.5)  +
  labs(title = "d",#title = "Conclusions of of reset SPRT over time",
       #subtitle = "H0 is true from t = 1:10; H1 is true from t =11:20",
       y = "Proportion of simulations",
       x = "Outbreak generation (t)",
       fill = "Decision") +
  my_theme()  
```

**Figure 4: Performance of reset SPRT when** $\boldsymbol{R_t}$ **decreases linearly**

# Next steps

* Individual based model
    + necessary to evaluate rules in reopening plans
    + can also use data to test performance of reset sprt on more complicated data
  
* Look into tranmission chain size as another data source for estimating R0 ([see Blumberg and Lloyd-Smith 2013](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002993))

* alternative model for estimating R_t
    + log transform time series and fit linear model. Use value of slope coefficient to make decision. Similar to approach in [White (2019)](https://doi.org/10.1093/biosci/biy144).

* read paper on detecting changes in poisson processes with linear drift in mean ([Zhao, Shu, Tsui (2014)](https://www.tandfonline.com/doi/ref/10.1080/00949655.2014.945933?scroll=top))
    + how can this be extended to negative binomial process?

<!-- # Assessment of 14-day rule -->

<!-- Before we can assess the performance of the 14-day rule we must translate the total cases per generation into cases per day. To do this we draw symptom onset dates from a log normal distribution `rlnorm(n = infected, meanlog = 1.621, sdlog = 0.418)` using parameters estimated by @lauer_incubation_2020. This procedure assumes that onset of symptoms is independent across cases which seems reasonable. However, it also assumes that all individuals in a generation were exposed at the exact same time. This assumption produces strong seasonality in cases per day because the probability of symptom onset is (see . To avoid this modeling artifact, I will need to also simulate the times of exposure in addition to onset of infectivity. An example of this is in @hellewell_feasibility_2020 ([code and r package](https://github.com/cmmid/ringbp)). -->

```{r inf_onset_times, eval = FALSE}
# point estimates from: Lauer et al 2020. Ann. Int. Med https://www.acpjournals.org/doi/10.7326/M20-0504
icub <- tibble(days = seq(0.5, 20, 0.5),
               onset_ln = dlnorm(x = days, meanlog = 1.621, sdlog = 0.418),
               onset_gamma = dgamma(x = days, shape =5.807, scale =0.948),
               onset_weibull = dweibull(x = days, shape = 2.453, scale = 6.258))

icub %>%
  pivot_longer(cols = 2:4,
               values_to = "probability",
               names_to = "distribution") %>%
  ggplot() +
  geom_line(aes(x = days, y = probability, col = distribution))
```


```{r assign_inf_onset, , out.width = "50%", fig.show = 'hold', eval = FALSE}
# simulate onset time of infectiousness
outbreak_inc <- df_break_low %>% 
  filter(simulation %in% 1:20) %>% # only do first 20 since the per_day and df calculations can take a while (setup dependency/cache later)
  rowwise() %>%
  mutate(inf_day = list(rlnorm(n = infected, meanlog = 1.621, sdlog = 0.418)) ) %>%
  # mutate(inf_day = list(rweibull(n = infected, shape = 2.453, scale = 6.258))) %>%
  # ok to truncate at 14 days since 
  # plnorm(q = 14, meanlog = 1.621, sdlog = 0.418) # = 0.992
  # summarize(max_incubation = sum(inf_day > 14)/infected) # at most ~ 1 % individuals ever have incubation times > 14 days
  mutate(inf_day = list((ceiling(inf_day))),
         inf_day = list(ifelse(inf_day > 14, 14, inf_day)))

# get number that were infected on day \in 1:14 in each time period  
per_day <-
  outbreak_inc %>% unnest(cols = c(inf_day)) %>%
  group_by(time, simulation) %>%
  count(inf_day) 

df <-
  per_day %>% complete(nesting(inf_day = 1:14), fill = list(n = 0)) %>%
  group_by(simulation) %>% mutate(day = row_number())
```
```{r plot_symptom_onset_per_day, out.width = "50%", fig.show = 'hold', eval = FALSE}
filter(df, simulation == 10) %>%
ggplot(aes(x = day, y = n, col = inf_day)) +
  geom_point() + viridis::scale_color_viridis() +
  labs(title = "Cases per day for the 10th simulation",
       caption = "Each generation is infected on same day and incubation is log-normal",
       y = "no. of new symptomatic cases") +
  my_theme()

ggplot(df, aes(x = day, y = n, group = simulation, col = as.factor(simulation))) +
  geom_line(alpha = 0.4) +
  labs(title = "Cases per day across 20 independent simulations",
       caption = "Each generation is infected on same day and incubation is log-normal",
       y = "no. of new symptomatic cases") +
  my_theme() +
  theme(legend.position = "none")
  

```

<!-- **Figure 4: Model assumptions cause spurious seasonality in new cases.** Perhaps modeling the time of infection will fix this problem. -->

# References {-}

<!-- https://stackoverflow.com/questions/51335125/adding-figures-and-tables-after-bibliography-in-rmarkdown -->

<div id="refs"></div>

\newpage 

<!-- # Key Personnel {-} -->

<!-- \newpage -->

<!-- # Budget -->
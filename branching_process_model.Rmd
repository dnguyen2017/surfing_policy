---
title: "Decision rules for reopening from COVID-19 lockdowns"
author: "David Nguyen"
date: "`r Sys.Date()`"
output: html_document
bibliography: bib_decision_rules.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

# Motivation

<!-- focus on the epidemiology and translate that into the relevant acceptance and rejection regions for re-opening. Don't worry too much about translating proposed re-opening guidelines into a hypothesis testing problem. It is enough to point out that they are trying to satisfy some objectives (NY has a very specific and concise list of objectives) -->

To safely determine when to lift COVID-19 lockdowns decision makers must know if the outbreak is sufficiently controlled and if there is adequate test and trace capacity to prevent a resurgence of cases following reopening. To support this need, criteria that should be satisfied to ensure it is safe to reopen have been proposed by policy, government, and academic entities [ex. @gottlieb_national_2020; @prevent_epidemics_adaptive_2020; @white_house_opening_2020; harvard]. A common criterion among reopening guidelines to determine if an outbreak is sufficiently controlled is a 14 day decrease in reported cases. This criterion was initially proposed in the American Enterprise Institute national reopening plan [page 3 @gottlieb_national_2020], possibly based on the reasoning that decreasing cases implies that the outbreak is controlled and the 14-day duration was chosen because most exposed individuals become infectious within 14 days [@lauer_incubation_2020]. To determine if there is adequate test and trace capacity some guidelines have proposed thresholds for the number of tests and contact tracers that are needed by population, e.g.,  a weekly average of 30 tests per 1,000 residents and 30 contact tracers per 100,000 residents ([NY Forward 2020](https://www.governor.ny.gov/sites/governor.ny.gov/files/atoms/files/NYForwardReopeningGuide.pdf)). These criteria and others are tests of the hypothesis that it unsafe (null) or safe (alternative) to reopen from lockdown. However, it is unclear what rejection region is implied by these tests, the performance of these hypothesis tests, or how they may compare to alternative tests. Consequently, it is unclear how statistically or epidemiologically justifiable these proposed reopening critera are. 
<!-- estimated that the 97.5 percentile of the incubation period distribution is 11.5 days (CI, 8.2 to 15.6 days)) -->

In this proposal I will address the problem of identifying when it is safe to phase out of lockdowns by developing epidemiologically justifiable hypothesis tests that can be used to set defensible criteria for reopening. 

<!-- 1. precisely identify the parameter region for which an epidemic is controlled enough to begin reopening -->
<!-- 2. develop a simple hypothesis test for identifying when a local epidemic is sufficiently controlled -->
<!-- 3. evaluate the performance of this hypothesis test and compare it the 14-day criteria proposed in reopening plans -->

<!-- A good decision rule should minimize the time needed to determine that the The performance of the 14-day decline in cases as a decision rule is unknown and there may exist more powerful decision rules.  -->

<!-- In this proposal I outline an approach to apply tools from sequential analysis to create statistically justified decision rules for guiding COVID-19 reopening decisions and will compare the performance of these rules with the heuristically chosen 14-day rule.  -->
<!-- For instance, what is the false detection rate of this rule? Given that an outbreak is sufficiently controlled to begin reopening, how many days will it take for -->

# Problem

Let $\theta \in \Theta$ be parameter that determines the evolution of an epidemic and let $\Theta_0$ be the set of parameters for which lockdown is necessary (acceptance region) and $\Theta_1$ be the set of parameters for which lockdown may be phased out (rejection region). The decision maker sequentially observes a stream of data $x_0, x_1, x_2,\ldots$ from the outbreak and must decide if the unsafe to reopen ($\theta \in \Theta_0$) or if it is safe to open ($\theta \in \Theta_1$). We seek to develop a sequential hypothesis test ($\phi(X_n)$) with a controlled type I error rate ($\alpha$)  to make this decision (i.e., $P(\text{reject } H_0 | \theta \in \Theta_0) \leq \alpha$). To solve this problem we must first define a model which describes the time-evolution of the outbreak, define the acceptance and rejection regions, construct a hypothesis tests.

### Model
A reasonable model for the initial phase of an outbreak (i.e. ignoring the depletion of susceptibles) is: 

\begin{align*}
Z_i & \sim NB(R(t), k)
\end{align*}

where $Z_i$ is the number of secondary cases produced by the $i$-th infected individual which follows a negative binomial distribution with mean $R(t)$ and dispersion $k$. Epidemiologically, $R(t)$ is the time-specific mean number of infections caused by an infectious individual and $k$ represents variability in infectiousness among individuals [@lloyd-smith_superspreading_2005]. Under the assumption that all $X_i$ are independent and identically distributed (iid) the total number of secondary infections is given by
<!-- (need to figure out how to write sum of neg bin under the mean-disp parameterization) -->

\begin{align*}
X_{t+1} & \sim \Sigma^{Z_t}_{i=1}X_i \\
            & \sim NB(\Sigma^{X_t}_{i=1} R_0, \Sigma^{X_t}_{i=1} k) \text{     (I think this is correct. still need to prove)}
\end{align*}

where the step size is chosen here to be the incubation period, 14 days (not sure how reasonable this is). 

```{r check_dist, echo = TRUE}
# numerically test claim that the previous two distributions are the same
test <- tibble("sum(X_i)" = replicate(10000, sum(rnbinom(n = 10, mu = 2, size = 0.16))),
       "NB(r0*x_t,k*x_t)" = replicate(10000, rnbinom(n = 1, mu = 10 * 2, size = 10 * 0.16)) ) %>%
  pivot_longer(cols = 1:2,
               names_to = "distribution",
               values_to = "output")
# ggplot(test) +
#   geom_density(aes(output, fill = distribution), alpha = 0.5)

ggplot(test) +
  stat_ecdf(aes(x = output, col = distribution)) +
  labs(title = "compare distributions")
```

```{r branching_model, eval = FALSE}
episim <- function (t, i_0, r0, disp, cont) {
  
  # initialize data frame
  out <- tibble("time" = t
                , "R0_true" = r0
                , "disp_true" = disp
                , "control" = cont
                , "infected" = i_0
  )
  
  # branching process simulations
  for (i in 2:length(t)) {
    out[i, "infected"] <- sum(rnbinom(n = unlist(out[i-1, "infected"]), # number infected in previous time step
                                      mu = (1 - unlist(out[i-1, "control"]))  * unlist(out[i-1, "R0_true"]), # (controlled) R
                                      size = unlist(out[i-1, "disp_true"]))) # dispersion
  }
  return(out)
}

# set params and init conds
t <- 1:20
r0 <- c(rep(2, 10), rep(0.9, 10))
disp <- 0.16 # sars-like dispersion
i_0 <- 10
cont <- 0

# simulate a single realization
outbreak <- episim(t = t, i_0 = i_0, r0 = r0, disp = disp, cont = cont)

```

### Define acceptance and rejection region
*Under construction*
To define the regions of parameter space for which it is safe ($\Theta_0$) or unsafe ($\Theta_1$) to phase out of lockdowns we must consider the effects of control on the outbreak. The branching process model can be modified to include the effect of lockdown (population-level control) and the effect of quarantine of contacts and isolation of positive-test cases (individual-level control).

The effect of lockdown can be modeled with the modification $R(t) = (1-c(t) R_0$ where $c(t)$ is the proportional decrease in infectiousness caused by lockdown and $R_0$ is the expected number of secondary infections in the absence of control in a fully susceptible population. This assumes that lockdown decreases contacts, and thus infectiousness, of all individuals equally.

individual level control *is more complicated, need to look at the appendix of @lloyd-smith_superspreading_2005 more closely*.

To define acceptance and rejection regions

  * explanation of impact of control type and magnitude on outbreak dynamics (e.g., extinction probability)
  * identify thresholds to determine $\Theta_0$ and $\Theta_1$

```{r likelihood, eval = FALSE}
# if it is true that for X_i ~ NB(mu, k) iid that sum(X_i) ~ NB(mu, sum(k)) then the P( x_{t+1} | x_t, theta ) is 
# dnbinom(x = x_now, mu = r0, size = x_prev*disp) because the X_i are iid
outbreak
r_h0 <- seq(1, 2, by = 0.1)
r_h1 <- seq(0, 1, by = 0.1)

obj_fn <- function(pars, input, dispersion) {
  dnbinom(x = input, mu = pars, size = size)
}

replicate(5, rnbinom(n = 1, mu = 2, size = 0.16))

optim(par = c(1), obj_fn(input = , dispersion = 0.16))

outbreak$like_h0 <- NA
outbreak$like_h1 <- NA

# want to find mle in parameter space of h0 and h1
# not currently right
for ( i in 2:nrow(outbreak)) {
   dnbinom(x = unlist(outbreak[i, "infected"]),
          mu = r_h0,
          size = unlist(outbreak[i, "disp_true"]))
}


```

### Construct hypothesis test
Making a slight modification to @phatarfod_sequential_1965 to allow for composite hypothesis testing of dependent observations let 

\begin{align*}
S_0 &= \log \left( \frac{p(x_0 | \theta \leq \theta_0 )}{p(x_0 | \theta > \theta_0)} \right) \\
S_r &= \log \left( \frac{p(x_r | x_{r-1}, \theta \leq \theta_0 )}{p(x_r | x_{r-1}, \theta > \theta_0)}\right), r \geq 1 \\
\end{align*}

then the sequential probability ratio test (SPRT) for the composite hypothesis test $H_0: \theta > \theta_0 \text{ against } H_1: \theta \leq \theta_0$ is

\begin{align*}
\phi(x_n) = 
    \begin{cases}
      \text{reject } H_0  & \Sigma^{n}_{0} S_r > \log(A)\\
      \text{accept } H_0 & \Sigma^{n}_{0} S_r \leq \log(B)
    \end{cases}
\end{align*}

The thresholds are set following Wald's method such that $A \sim \frac{1-\beta}{\alpha}$ and $B \sim \frac{\beta}{1-\alpha}$ where $\alpha$ and $\beta$ is the probabilities of type I and II error, respectively.

In our case, the parameter ($\theta$) of interest is $R(t)$. When $S_r > \log(A)$ the decision maker concludes that it is unsafe to reopen and when $S_r \leq \log (B)$. This should have an advantage over the criteria that have been proposed on purely heuristic grounds since the decision maker is able to set choose the type I and II error rates whereas the error rates of the proposed criteria are uncontrolled and unknown. Another likely justification is that the SPRT may be the optimal test for this problem class (it is for the case where observations are iid, not sure about the markov dependence case).


# References
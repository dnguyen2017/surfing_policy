---
title: "Decision rules for reopening from COVID-19 lockdowns"
author: "David Nguyen"
date: "`r Sys.Date()`"
output: html_document
bibliography: bib_decision_rules.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

# Motivation

<!-- focus on the epidemiology and translate that into the relevant acceptance and rejection regions for re-opening. Don't worry too much about translating proposed re-opening guidelines into a hypothesis testing problem. It is enough to point out that they are trying to satisfy some objectives (NY has a very specific and concise list of objectives) -->

To safely determine when to lift COVID-19 lockdowns decision makers must know if the outbreak is sufficiently controlled and if there is adequate test and trace capacity to prevent a resurgence of cases following reopening. To support this need, criteria that should be satisfied to ensure it is safe to reopen have been proposed by policy, government, and academic entities [ex. @gottlieb_national_2020, @prevent_epidemics_adaptive_2020, @white_house_opening_2020]. A common criterion among reopening guidelines to determine if an outbreak is sufficiently controlled is a 14 day decrease in reported cases. This criterion was initially proposed in the American Enterprise Institute national reopening plan [page 3 @gottlieb_national_2020], possibly based on the reasoning that decreasing cases implies that the outbreak is controlled and the 14-day duration was chosen because most exposed individuals become infectious within 14 days [@lauer_incubation_2020]. To determine if there is adequate test and trace capacity some guidelines have proposed thresholds for the number of tests and contact tracers that are needed by population, e.g.,  a weekly average of 30 tests per 1,000 residents and 30 contact tracers per 100,000 residents ([NY Forward 2020](https://www.governor.ny.gov/sites/governor.ny.gov/files/atoms/files/NYForwardReopeningGuide.pdf)). These criteria and others are tests of the hypothesis that it unsafe (null) or safe (alternative) to reopen from lockdown. However, it is unclear what rejection region is implied by these tests, the performance of these hypothesis tests, or how they may compare to alternative tests. Consequently, it is unclear how statistically or epidemiologically justifiable these proposed reopening critera are. 
<!-- estimated that the 97.5 percentile of the incubation period distribution is 11.5 days (CI, 8.2 to 15.6 days)) -->

In this proposal I will address the problem of identifying when it is safe to phase out of lockdowns by developing epidemiologically justifiable hypothesis tests that can be used to set defensible criteria for reopening. 

<!-- 1. precisely identify the parameter region for which an epidemic is controlled enough to begin reopening -->
<!-- 2. develop a simple hypothesis test for identifying when a local epidemic is sufficiently controlled -->
<!-- 3. evaluate the performance of this hypothesis test and compare it the 14-day criteria proposed in reopening plans -->

<!-- A good decision rule should minimize the time needed to determine that the The performance of the 14-day decline in cases as a decision rule is unknown and there may exist more powerful decision rules.  -->

<!-- In this proposal I outline an approach to apply tools from sequential analysis to create statistically justified decision rules for guiding COVID-19 reopening decisions and will compare the performance of these rules with the heuristically chosen 14-day rule.  -->
<!-- For instance, what is the false detection rate of this rule? Given that an outbreak is sufficiently controlled to begin reopening, how many days will it take for -->

# Problem

Let $\theta \in \Theta$ be parameter that determines the evolution of an epidemic and let $\Theta_0$ be the set of parameters for which lockdown is necessary (acceptance region) and $\Theta_1$ be the set of parameters for which lockdown may be phased out (rejection region). The decision maker sequentially observes a stream of data $x_0, x_1, x_2,\ldots$ from the outbreak and must decide if the unsafe to reopen ($\theta \in \Theta_0$) or if it is safe to open ($\theta \in \Theta_1$). We seek to develop a sequential hypothesis test ($\phi(X_n)$) with a controlled type I error rate ($\alpha$)  to make this decision (i.e., $P(\text{reject } H_0 | \theta \in \Theta_0) \leq \alpha$). To solve this problem we must first define a model which describes the time-evolution of the outbreak, define the acceptance and rejection regions, construct a hypothesis tests.

### Model
A reasonable model for the initial phase of an outbreak (i.e. ignoring the depletion of susceptibles) is: 

\begin{align*}
Z_i & \sim NB(R(t), k)
\end{align*}

where $Z_i$ is the number of secondary cases produced by the $i$-th infected individual which follows a negative binomial distribution with mean $R(t)$ and dispersion $k$. Epidemiologically, $R(t)$ is the time-specific mean number of infections caused by an infectious individual and $k$ represents variability in infectiousness among individuals [@lloyd-smith_superspreading_2005]. Under the assumption that all $X_i$ are independent and identically distributed (iid) the total number of secondary infections is given by

\begin{align*}
X_{t+1} & \sim \Sigma^{Z_t}_{i=1}X_i \\
            & \sim NB(\Sigma^{X_t}_{i=1} R(t), \Sigma^{X_t}_{i=1} k)
\end{align*}

where the step size is chosen here to be the incubation period, 14 days (not sure how reasonable this is). 

```{r check_dist, eval = FALSE}
# numerically test claim that the previous two distributions are the same
test <- tibble("sum(X_i)" = replicate(10000, sum(rnbinom(n = 10, mu = 2, size = 0.16))),
       "NB(r0*x_t,k*x_t)" = replicate(10000, rnbinom(n = 1, mu = 10 * 2, size = 10 * 0.16)) ) %>%
  pivot_longer(cols = 1:2,
               names_to = "distribution",
               values_to = "output")
# ggplot(test) +
#   geom_density(aes(output, fill = distribution), alpha = 0.5)

ggplot(test) +
  stat_ecdf(aes(x = output, col = distribution)) +
  labs(title = "compare distributions")
```

```{r branching_model}
episim <- function (t, i_0, r0, disp, cont, nsim) {
  
  # init list to store each simulation
  out_list <- vector("list", nsim)
  
  for (k in seq_along(out_list)) {
    # initialize data frame
    out <- tibble("time" = t
                  , "R0_true" = r0
                  , "disp_true" = disp
                  , "control" = cont
                  , "infected" = NA
                  , "simulation" = k)
    out[1, "infected"] <- i_0
    
    # branching process simulations
    for (i in 2:length(t)) {
      out[i, "infected"] <- sum(rnbinom(n = unlist(out[i-1, "infected"]), # number infected in previous time step
                                        mu = (1 - unlist(out[i-1, "control"]))  * unlist(out[i-1, "R0_true"]), # (controlled) R
                                        size = unlist(out[i-1, "disp_true"]))) # dispersion
      if(out[i, "infected"] == 0) break
    }
    # save in out_list
    out_list[[k]] <- out %>% filter(!is.na(infected))
  }
  return(bind_rows(out_list))
}

# set params and init conds
t <- 1:20
r0 <- c(rep(2, 10), rep(0.9, 10)) # lockdown starts at t = 11
disp <- 0.14 # median of posterior estimate of k for SARS-CoV-2 (Riou and Althaus 2020)
i_0 <- 10
cont <- 0
n <- 100

# simulate
outbreak <- episim(t = t, i_0 = i_0, r0 = r0, disp = disp, cont = cont, nsim = n)
```

Here are some example simulations of this model. Control is implemented at the 11^th^ time step.

```{r plot_example_branching_proc, out.width = "50%", fig.show = 'hold', eval = FALSE}
# Plot simulations
ggplot(outbreak, aes(x = time, y = infected, group = simulation, col = as.factor(simulation) )) +
  geom_line(alpha = 0.7) +
  scale_y_log10() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Epidemic trajectories"
       , y = "log10(infected)"
       , x = "incubation periods")

outbreak %>% group_by(simulation) %>% summarize(total_infected = sum(infected)) %>% 
ggplot() +
  geom_density(aes(x = total_infected) )+
  scale_x_log10() +
  labs(title = "Outbreak size distribution" 
         ,x = "log10(total infected)")

```


### Define acceptance and rejection region
*Under construction*
To define the regions of parameter space for which it is safe ($\Theta_0$) or unsafe ($\Theta_1$) to phase out of lockdowns we must consider the effects of control on the outbreak. The branching process model can be modified to include the effect of lockdown (population-level control) and the effect of quarantine of contacts and isolation of positive-test cases (individual-level control).

The effect of lockdown can be modeled with the modification $R(t) = (1-c(t) R_0$ where $c(t)$ is the proportional decrease in infectiousness caused by lockdown and $R_0$ is the expected number of secondary infections in the absence of control in a fully susceptible population. This assumes that lockdown decreases contacts, and thus infectiousness, of all individuals equally.

individual level control *is more complicated, need to look at the appendix of @lloyd-smith_superspreading_2005 more closely*.

To define acceptance and rejection regions

  * explanation of impact of control type and magnitude on outbreak dynamics (e.g., extinction probability)
  * identify thresholds to determine $\Theta_0$ and $\Theta_1$

```{r extinction_prob}
# per supplement of lloyd-smith et al. 2005:
# The probability that the population is extinct by the nth generation is denoted q and the pgf is denoted g(s).
# Then q = g_n(q)  as n -> inf and q is bounded [0,1]
# where g(s) = (1 + (r0/k) * (1 - s*))^(-k), for the NB model
# and g_0(s) = s, g_1(s) = g(s), ..., g_n+1(s) = g(g_n(s))

gfun <- function (s, r0, k) { # g(s; r0, k) 
  return((1 + (r0/k) * (1 - s))^(-k))
}

niteration <- 10^2
# q <- vector("numeric", length = niteration + 1)
# q[1] <- 0 # s
# 
# # set values of R to calculate extinction probability
# r0_vec <- seq(0.1, 5, by = 0.1)
# q_list <- vector("list", length = length(r0_vec))
# 
# for (i in seq_along(r0_vec)) {
#   # init vector to store values of q
#   q <- vector("numeric", length = niteration + 1)
#   q[1] <- 0 # g_0(0)
#   
#   for (j in 1:(niteration + 1)) {
#     q[j + 1] <- gfun(q[j], r0_vec[i], 0.16) # calculate g_1(0), ..., g_n(0) 
#   }
#   
#   q_list[[i]] <- tibble(n = 0:(niteration + 1),
#                         pext = q,
#                         r0 = r0_vec[i])
# }
# 
# pext_df <- bind_rows(q_list)
# 
# # verify convergence of q (all lines should asymptote)
# # ggplot(pext_df, aes(x = n, y = pext, col = r0, group = r0)) +
# #   geom_line() + viridis::scale_color_viridis()
# 
# pext_df %>%
#   group_by(r0) %>%
#   summarize(p_extinction = max(pext)) %>%
#   ggplot() +
#   geom_line(aes(x = r0, y = p_extinction), size = 1.5) +
#   geom_vline(aes(xintercept = 1), linetype = "dashed") +
#   lims(y = c(0,1)) +
#   labs(title = "Probability of stochastic extinction of outbreak",
#        x = "Basic reproductive number (R0)",
#        y = "probability of extinction (q)",
#        caption = "dispersion: k = 0.16")

# estimates of k for SARS-CoV-2 
k_vec <- c(0.54, 0.14, 6.95, # median and 90 % highest density interval from Riou and Althaus (2020) Euro Surveill
           0.1, 0.05, 0.2)   # endo et al. 2020 wellcome open research
r0_vec <- seq(0.1, 5, by = 0.1)
q_list <- vector("list", length(k_vec)*length(r0_vec))

x <- 1
for (i in seq_along(k_vec)) {
  # init vector to store values of q
  for (j in seq_along(r0_vec)) {
    q <- vector("numeric", length = niteration + 1)
    q[1] <- 0 # g_0(0)
    # calculate g_1(0), ..., g_n(0)
    for (k in 1:(niteration + 1)) {
      q[k + 1] <- gfun(q[k], r0_vec[j], k_vec[i])  
    }
  # create df and store in q_list
  q_list[[x]] <- tibble(n = 0:(niteration + 1),
                        pext = q,
                        k = k_vec[i],
                        r0 = r0_vec[j])
  x <- x + 1
  }
}

```

```{r compare_lloydsmith, eval = FALSE}
k_vec <- c(0.01, 0.1, 0.5, 1, 4, 10^6) # last one is "infinity"
r0_vec <- seq(0.1, 5, by = 0.1)
q_list <- vector("list", length(k_vec)*length(r0_vec))

x <- 1
for (i in seq_along(k_vec)) {
  # init vector to store values of q
  for (j in seq_along(r0_vec)) {
    q <- vector("numeric", length = niteration + 1)
    q[1] <- 0 # g_0(0)
    # calculate g_1(0), ..., g_n(0)
    for (k in 1:(niteration + 1)) {
      q[k + 1] <- gfun(q[k], r0_vec[j], k_vec[i])  
    }
  # create df and store in q_list
  q_list[[x]] <- tibble(n = 0:(niteration + 1),
                        pext = q,
                        k = k_vec[i],
                        r0 = r0_vec[j])
  x <- x + 1
  }
}

q_list %>% bind_rows() %>%
  group_by(k, r0) %>%
  summarize(p_extinction = max(pext)) %>%
  ggplot(aes(x = r0, y = p_extinction, group = k, col = as.factor(k) )) +
  # ggplot(aes(x = n, y = pext, col = k, group = k)) +
  geom_line(size = 2) + viridis::scale_color_viridis(discrete = T) +
  theme_minimal() +
  labs(title = "Probability of stochastic extinction of an outbreak",
       subtitle = "Successfully matches plot 2.b from Lloyd-Smith et al. 2005",
       x = "Basic reproductive number (R0)",
       y = "Probability of extinction (q)",
       col = "dispersion (k)")
  

```


```{r simulate_final_size, cache = TRUE}
# total infected after n generations
rvals <- seq(0.1, 5, by = 0.1)
finalsize_list <- vector("list", length = length(rvals))
set.seed(123)
for ( i in seq_along(rvals)) {
  finalsize_list[[i]] <- episim(t = 1:10, i_0 = 10, r0 = rvals[i], disp = disp, cont = cont, 
                                nsim = 50)
}

finalsize_df <- bind_rows(finalsize_list)

finalsize_df <-
  finalsize_df %>%
  group_by(R0_true, simulation) %>%
  summarize(total = sum(infected))
```

```{r extinction_plot, out.width = "50%", fig.show = 'hold'}
# probability of extinction
extinction_df <- 
  q_list %>% bind_rows() %>%
group_by(k, r0) %>%
summarize(p_extinction = max(pext)) %>%
mutate(estimate = case_when(k == 0.1 | k == 0.54 ~ "a.median",
                            k == 0.05 | k == 0.14 | k == 6.95 | k == 0.2 ~ "b.interval"),
       source = case_when(k == 0.05 | k == 0.1 | k == 0.2 ~ "Endo et al. (2020)",
                          k == 0.14 | k == 0.54 | k == 6.95 ~ "Riou & Althaus (2020)"))

# pal <- c("#440154FF", "#55C667FF") # set manual color palette
extinction_df %>%
  mutate(label = if_else(r0 == max(r0), paste0("k = ",k), NA_character_)) %>%
  ggplot(aes(x = r0, y = p_extinction, group = k, linetype = source)) +#, col = source, linetype = estimate)) +
  geom_line(size = 1) + 
  #viridis::scale_color_viridis(discrete = T) +
  #scale_color_manual(values = pal) +
  geom_vline(aes(xintercept = 1), linetype = "dashed", size = 1.5, col = "red") +
  ylim(0,1) +
  theme_minimal() + theme(legend.position = "none") +
  labs(title = "a. Probability of stochastic extinction of an outbreak",
       x = "Basic reproductive number (R0)",
       y = "Probability of extinction (q)",
       col = "dispersion (k)") +
  ggrepel::geom_text_repel(aes(label = label), nudge_y = 0.05, nudge_x = 1, na.rm = TRUE)


extinction_df %>%
  #filter(source == "Endo et al. (2020)") %>%
  filter(k == 0.1) %>%
  expand_grid(z0 = c(1, 10, 50, 100)) %>%
  mutate(p_extinction = p_extinction^z0) %>%
  mutate(label = if_else(near(r0, 1.5), paste("Z[0] == ~" ,z0), NA_character_)) %>%
  ggplot(aes(x = r0, y = p_extinction, group = z0)) +#, col = source, linetype = estimate)) +
  geom_line(size = 1) +
  geom_vline(aes(xintercept = 1), linetype = "dashed", size = 1, col = "red") +
  ylim(0,1) +
  theme_minimal() + theme(legend.position = "none") +
  labs(title = "b.",
       x = "Basic reproductive number (R0)",
       y = "Probability of extinction (q)",
       col = "dispersion (k)") +
  ggrepel::geom_text_repel(aes(label = label), nudge_y = 0.05, nudge_x = 1, na.rm = TRUE, parse = TRUE)


```
**Figure 1: a. Probability of a stochastic extinction of an outbreak.** *add explanation of threshold at r0 = 1 and why low k (high dispersion) has higher probability of extinction compared to low dispersion)* Values of dispersion (k) are estimates from two studies of SARS-CoV-2 (solid lines are median and dashed lines are interval estimates; green - median 0.54 (0.14 - 6.54 90 % HDI) @riou_pattern_2020; purple - median 0.1 (0.05 - 0.2, 95 % CrI) @endo_estimating_2020). **b. Comparison of total cases by 10th generation.** Each point is an independent simulation. 50 simulations were run for each value of $R_0$ and k = 0.16 for all simulations.

```{r zn_plot}
# final size
finalsize_df %>%
  ggplot() +
  geom_jitter(aes(x = R0_true, y = total), alpha = 0.5) +
  scale_y_log10() +
  theme_minimal() +
  geom_vline(aes(xintercept = 1), linetype = "dashed", size = 1.5, col = "red") +
  geom_hline(aes(yintercept = 10), size = 1.5) +
  labs(title = "b. Distribution of total infections by 10th generation",
       y = "Total infections",
       x = "Basic reproductive number (R0)")

```


<!-- should use the two median estimates of k for the size at n-th generation plot -->

```{r likelihood, results = 'hide'}
# if it is true that for X_i ~ NB(mu, k) iid that sum(X_i) ~ NB(sum(mu), sum(k)) then the P( x_{t+1} | x_t, theta ) is 
# dnbinom(x = x1, mu = x0 * r0, size = x0 * disp) because the X_i are iid

objfn <- function(r, x1, x0, dispersion) {
  # likelihood( x1 | x0, r, dispersion )
  dnbinom(x = x1, mu = x0*r, size = x0*dispersion)
}

mean(replicate(100, rnbinom(n = 1, mu = 10*2, size = 10*0.16)))

#  find restricted MLE under h_0 or h_1
likelihood <- function (x_prev, x_now, disp, interval = c(), guess = 1.5) {
  opt <- optim(par = guess, fn = objfn, control = list(fnscale = -1), 
      method = "Brent", lower = interval[1], upper = interval[2],
      x0 = x_prev, x1 = x_now, dispersion = disp)
  return(opt$value)
}

likelihood(x_prev = 10, x_now = 20, disp = 0.16, interval = c(1, 2.5))

```

```{r calculate_sprt, results = 'hide'}
set.seed(123)
testdf <- episim(t = t, i_0 = i_0, r0 = r0, disp = disp, cont = cont, nsim = 1)

testdf$lik0 <- NA
testdf$lik1 <- NA

for (i in 2:nrow(testdf)) {
  # observed states and known pars
  x0 <- unlist(testdf[i-1, "infected"])
  x1 <- unlist(testdf[i, "infected"])
  disp <- unlist(testdf[i-1, "disp_true"])
  
  # parameter space
  theta0 <- c(1 + 10^-6, 2.5) # acceptance
  theta1 <- c(0, 1)           # rejection
  # init guess for optim
  guess0 <- 1.5
  guess1 <- 0.5
  
  # likelihood calculation
  testdf[i,"lik0"] <- likelihood(x_prev = x0, x_now = x1, disp = disp,  # p(x_now | x_prev, disp, theta \in Theta_0)
                     interval = theta0, guess = guess0)
  testdf[i,"lik1"] <- likelihood(x_prev = x0, x_now = x1, disp = disp,  # p(x_now | x_prev, disp, theta \in Theta_1)
                     interval = theta1, guess = guess1)
}
 
test_sprt <- testdf %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.05)/ 0.05),
         b = log(0.05/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))

sprt_reset <- vector("numeric", length = nrow(test_sprt))
decision_reset <- vector("character", length = nrow(test_sprt))

sr <- test_sprt$sr
a <- unique(test_sprt$a)
b <- unique(test_sprt$b)
  
sprt_reset[1] <- 0
decision_reset[1] <- "?"

for (i in 2:nrow(test_sprt)) {
  
  # if no decision has been made, update SPRT
  # otherwise, reset the SPRT
  ifelse(decision_reset[i] == "?",
         sprt_reset[i] <- sprt_reset[i-1] + sr[i],
         sprt_reset[i] <- sr[i])
  
  # if SPRT has crossed bounds, make decision
  # otherwise continue sampling
  ifelse(sprt_reset[i] < b
         ,{decision_reset[i] <- "H0"}#; sprt_reset[i+1] <- sr[i+1]; next}
         ,ifelse(sprt_reset[i] > a
                 ,{decision_reset[i] <- "H1"}#; sprt_reset[i+1] <- sr[i+1]; next}
                 ,{decision_reset[i] <- "?"})
         )
}

test_sprt$sprt_reset <- sprt_reset
test_sprt$decision_reset <- decision_reset

test_sprt
```

### Construct hypothesis test
Making a slight modification to @phatarfod_sequential_1965 to allow for composite hypothesis testing of dependent observations let 

\begin{align*}
S_0 &= \log \left( \frac{p(x_0 | \theta \leq \theta_0 )}{p(x_0 | \theta > \theta_0)} \right) \\
S_r &= \log \left( \frac{p(x_r | x_{r-1}, \theta \leq \theta_0 )}{p(x_r | x_{r-1}, \theta > \theta_0)}\right), r \geq 1 \\
\end{align*}

then the sequential probability ratio test (SPRT) for the composite hypothesis test $H_0: \theta > \theta_0 \text{ against } H_1: \theta \leq \theta_0$ is

\begin{align*}
\phi(x_n) = 
    \begin{cases}
      \text{reject } H_0  & \Sigma^{n}_{r = 0} S_r > \log(A)\\
      \text{accept } H_0 & \Sigma^{n}_{r = 0} S_r \leq \log(B)
    \end{cases}
\end{align*}

The thresholds are set following Wald's method such that $A \sim \frac{1-\beta}{\alpha}$ and $B \sim \frac{\beta}{1-\alpha}$ where $\alpha$ and $\beta$ is the probabilities of type I and II error, respectively.

In our case, the parameter ($\theta$) of interest is $R(t)$. When $S_r > \log(A)$ the decision maker concludes that it is unsafe to reopen and when $S_r \leq \log (B)$. This should have an advantage over the criteria that have been proposed on purely heuristic grounds since the decision maker is able to set choose the type I and II error rates whereas the error rates of the proposed criteria are uncontrolled and unknown. Another likely justification is that the SPRT may be the optimal test for this problem class (it is for the case where observations are iid, not sure about the markov dependence case).

```{r plot_sprt, out.width = "50%", fig.show = 'hold'}
test_sprt %>%
  ggplot() +
  geom_line(aes(x = time, y = sprt)) +
  geom_point(aes(x = time, y = sprt, col = decision), size = 2) +
  geom_hline(aes(yintercept = a), linetype = "dashed") +
  geom_hline(aes(yintercept = b), linetype = "dashed") +
  labs(title = "SPRT without resetting",
       caption = "Previous confirmation of H0 overwhelms new information that is consistent with H1")

test_sprt %>%
  ggplot() +
  geom_line(aes(x = time, y = sprt_reset)) +
  geom_point(aes(x = time, y = sprt_reset, col = decision_reset), size = 2) +
  geom_hline(aes(yintercept = a), linetype = "dashed") +
  geom_hline(aes(yintercept = b), linetype = "dashed") +
  labs(title = "SPRT with resetting")
```


```{r inf_onset_times, eval = FALSE}
# point estimates from: Lauer et al 2020. Ann. Int. Med https://www.acpjournals.org/doi/10.7326/M20-0504
icub <- tibble(days = seq(0.5, 20, 0.5),
               onset_ln = dlnorm(x = days, meanlog = 1.621, sdlog = 0.418),
               onset_gamma = dgamma(x = days, shape =5.807, scale =0.948),
               onset_weibull = dweibull(x = days, shape = 2.453, scale = 6.258))

icub %>%
  pivot_longer(cols = 2:4,
               values_to = "probability",
               names_to = "distribution") %>%
  ggplot() +
  geom_line(aes(x = days, y = probability, col = distribution))
```

# Assessment of 14-day rule

Before we can assess the performance of the 14-day rule we must translate the total cases per generation into cases per day. TO do so, we draw symptom onset dates from a log normal distribution `rlnorm(n = infected, meanlog = 1.621, sdlog = 0.418)` using parameters estimated by @lauer_incubation_2020. This procedure assumes that onset of symptoms is independent across cases which seems reasonable. However, it also assumes that all individuals in a generation were exposed at the exact same time, which creates strong biweekly seasonality in cases per day as seen in the following plots. To avoid this modeling artifact, I will need to also simulate the times of exposure in addition to onset of infectivity. An example of this is in @hellewell_feasibility_2020 ([code and r package](https://github.com/cmmid/ringbp)).

```{r assign_inf_onset, , out.width = "50%", fig.show = 'hold'}
# simulate onset time of infectiousness
outbreak_inc <- outbreak %>% 
  filter(simulation %in% 1:20) %>% # only do first 20 since the per_day and df calculations can take a while (setup dependency/cache later)
  rowwise() %>%
  mutate(inf_day = list(rlnorm(n = infected, meanlog = 1.621, sdlog = 0.418)) ) %>%
  # ok to truncate at 14 days since 
  # plnorm(q = 14, meanlog = 1.621, sdlog = 0.418) # = 0.992
  # summarize(max_incubation = sum(inf_day > 14)/infected) # at most ~ 1 % individuals ever have incubation times > 14 days
  mutate(inf_day = list((ceiling(inf_day))),
         inf_day = list(ifelse(inf_day > 14, 14, inf_day)))

# get number that were infected on day \in 1:14 in each time period  
per_day <-
  outbreak_inc %>% unnest(cols = c(inf_day)) %>%
  group_by(time, simulation) %>%
  count(inf_day) 

df <-
  per_day %>% complete(nesting(inf_day = 1:14), fill = list(n = 0)) %>%
  group_by(simulation) %>% mutate(day = row_number())
```


```{r plot_symptom_onset_per_day, out.width = "50%", fig.show = 'hold'}
filter(df, simulation == 10) %>%
ggplot(aes(x = day, y = n, col = inf_day)) +
  geom_point() + viridis::scale_color_viridis() +
  labs(title = "Cases per day for the 10th simulation",
       caption = "Each generation is infected on same day and incubation is log-normal",
       y = "no. of individuals that become symptomatic") +
  theme_minimal()

ggplot(df, aes(x = day, y = n, group = simulation, col = as.factor(simulation))) +
  geom_line(alpha = 0.4) +
  labs(title = "Cases per day across 20 independent simulations",
       caption = "Each generation is infected on same day and incubation is log-normal",
       y = "no. of individuals that become symptomatic") +
  theme_minimal() +
  theme(legend.position = "none")
  

```


# References
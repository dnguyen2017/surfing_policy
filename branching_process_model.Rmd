---
title: "epidemic branching processes"
author: "David Nguyen"
date: "May 30, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

# Problem

<!-- focus on the epidemiology and translate that into the relevant acceptance and rejection regions for re-opening. Don't worry too much about translating proposed re-opening guidelines into a hypothesis testing problem. It is enough to point out that they are trying to satisfy some objectives (NY has a very specific and concise list of objectives) -->

To safely determine when to lift COVID-19 lockdowns decision makers must know if the outbreak is sufficiently controlled and if there is adequate test and trace capacity to prevent a resurgence of cases following reopening. To support this need, criteria for that must be satisfied before reopoening have been proposed by policy, government, and academic entities (ex. AEI, pandemic preparedness, white house, new york state, harvard). A common criterion among reopening guidelines to determine if an outbreak is sufficiently controlled is a 14 day decrease in reported cases. This criterion was initially proposed in the American Enterprise Institute national reopening plan (Gottleib et al. 2020, page 3), possibly based on the reasoning that decreasing cases implies that the outbreak is controlled and the 14-day duration was chosen because most exposed individuals become infectious within 14 days (Lauer et al. 2020). To determine if there is adequate test and trace capacity some guidelines have proposed thresholds for the number of tests and contact tracers that are needed by population, e.g.,  a weekly average of 30 tests per 1,000 residents and 30 contact tracers per 100,000 residents (NY Forward 2020). These criteria and others are tests of the hypothesis that it unsafe (null) or safe (alternative) to reopen from lockdown. However, it is unclear what rejection region is implied by these tests, the performance of these hypothesis tests, or how they may compare to alternative tests. Consequently, it is unclear how statistically or epidemiologically justifiable these proposed reopening critera are. 
<!-- estimated that the 97.5 percentile of the incubation period distribution is 11.5 days (CI, 8.2 to 15.6 days)) -->

In this proposal I will approach the problem of identifying when it is safe to phase out of lockdowns by developing epidemiologically justifiable hypothesis tests. 

1. precisely identify the parameter region for which an epidemic is controlled enough to begin reopening
2. develop a simple hypothesis test for identifying when a local epidemic is sufficiently controlled
3. evaluate the performance of this hypothesis test and compare it the 14-day criteria proposed in reopening plans

A good decision rule should minimize the time needed to determine that the The performance of the 14-day decline in cases as a decision rule is unknown and there may exist more powerful decision rules. 

In this proposal I outline an approach to apply tools from sequential analysis to create statistically justified decision rules for guiding COVID-19 reopening decisions and will compare the performance of these rules with the heuristically chosen 14-day rule. 
<!-- For instance, what is the false detection rate of this rule? Given that an outbreak is sufficiently controlled to begin reopening, how many days will it take for -->

# Model
A reasonable model for the initial phase of an outbreak (i.e. ignoring the depletion of susceptibles) is: 

\begin{align*}
X_i & \sim NB(R(t), k)
\end{align*}

where $X_i$ is the number of secondary cases produced by the $i$-th infected individual which follows a negative binomial distribution with mean $R(t)$ and dispersion $k$. Epidemiologically, $R(t)$ is the time-specific mean number of infections caused by an infectious individual and $k$ represents variability in infectiousness among individuals (Lloyd-Smith et al. 2005). Here, $R(t)$ is a function of the time-varying control $c(t)$ which is the proportional reduction in transmission under an active lock-down policy and $R_0$ the expected number of infections caused by an infectious individual in the absence of control measures in a fully-susceptible population. Under the assumption that all $X_i$ are independent and identically distributed (iid) the total number of secondary infections is given by
<!-- (need to figure out how to write sum of neg bin under the mean-disp parameterization) -->

\begin{align*}
X_{t+1} & \sim \Sigma^{X_t}_{i=1}X_i \\
            & \sim NB(R_0, \Sigma^{X_t}_{i=1} k) \text{     (I think this is correct)}
\end{align*}

```{r check_dist, echo = TRUE}
tibble(x = replicate(5, sum(rnbinom(n = 10, mu = 2, size = 0.16))),
       y = replicate(5, rnbinom(n = 1, mu = 2, size = 10 * 0.16)) )
```

Nevermind, it is absolutely not the case that $X_{t+1} \sim NB(R_0, \Sigma^{X_t}_{i=1} k)$

where the step size is chosen here to be the incubation period, 14 days (not sure how reasonable this is). 

```{r branching_model, eval = FALSE}
episim <- function (t, i_0, r0, disp, cont) {
  
  # initialize data frame
  out <- tibble("time" = t
                , "R0_true" = r0
                , "disp_true" = disp
                , "control" = cont
                , "infected" = i_0
  )
  
  # branching process simulations
  for (i in 2:length(t)) {
    out[i, "infected"] <- sum(rnbinom(n = unlist(out[i-1, "infected"]), # number infected in previous time step
                                      mu = (1 - unlist(out[i-1, "control"]))  * unlist(out[i-1, "R0_true"]), # (controlled) R
                                      size = unlist(out[i-1, "disp_true"]))) # dispersion
  }
  return(out)
}

# set params and init conds
t <- 1:20
r0 <- c(rep(2, 10), rep(0.9, 10))
disp <- 0.16 # sars-like dispersion
i_0 <- 10
cont <- 0

# simulate a single realization
outbreak <- episim(t = t, i_0 = i_0, r0 = r0, disp = disp, cont = cont)

```

```{r likelihood, eval = FALSE}
# if it is true that for X_i ~ NB(mu, k) iid that sum(X_i) ~ NB(mu, sum(k)) then the P( x_{t+1} | x_t, theta ) is 
# dnbinom(x = x_now, mu = r0, size = x_prev*disp) because the X_i are iid
outbreak
r_h0 <- seq(1, 2, by = 0.1)
r_h1 <- seq(0, 1, by = 0.1)

obj_fn <- function(pars, input, dispersion) {
  dnbinom(x = input, mu = pars, size = size)
}

replicate(5, rnbinom(n = 1, mu = 2, size = 0.16))

optim(par = c(1), obj_fn(input = , dispersion = 0.16))

outbreak$like_h0 <- NA
outbreak$like_h1 <- NA

# want to find mle in parameter space of h0 and h1
# not currently right
for ( i in 2:nrow(outbreak)) {
   dnbinom(x = unlist(outbreak[i, "infected"]),
          mu = r_h0,
          size = unlist(outbreak[i, "disp_true"]))
}


```


Making a slight modification to Phatarfod (1965) to allow for composite hypothesis testing let 

\begin{align*}
Z_0 &= \log \left( \frac{p(x_0 | \theta \leq \theta_0 )}{p(x_0 | \theta > \theta_0)} \right) \\
Z_r &= \log \left( \frac{p(x_r | x_{r-1}, \theta \leq \theta_0 )}{p(x_r | x_{r-1}, \theta > \theta_0)}\right), r \geq 1 \\
\end{align*}

then the sequential probability ratio test (SPRT) for the composite hypothesis test $H_0: \theta > \theta_0 \text{ against } H_1: \theta \leq \theta_0$ is

\begin{align*}
\phi(x_n) = 
    \begin{cases}
      \text{reject } H_0  & \Sigma^{n}_{0} Z_r > \log(A)\\
      \text{accept } H_0 & \Sigma^{n}_{0} Z_r \leq \log(B)
    \end{cases}
\end{align*}

The thresholds are set following Wald's method $A \sim \frac{1-\beta}{\alpha}$ and $B \sim \frac{\beta}{1-\alpha}$ where $\alpha$ and $\beta$ is the probabilities of type I and II error, respectively.

In our case, the parameter ($\theta$) of interest is $R(t)$.
